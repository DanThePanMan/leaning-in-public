{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b876930",
   "metadata": {},
   "source": [
    "# CRIPS-DM\n",
    "Cross industry standard proccess in data mining\n",
    "\n",
    "for ml projects, we need to:\n",
    "- understand the problem\n",
    "- collect the data\n",
    "- train the model\n",
    "- use it\n",
    "\n",
    "methodologies like CRIPS-DM helps make it more managable\n",
    "\n",
    "## using the spam detection example\n",
    "\n",
    "This is how CRIPS-DM works\n",
    "![CRIPS-DM](./images/CRISP-DM.png)\n",
    "\n",
    "1. business understanding: \n",
    "   - do we need ML here?\n",
    "   - what is the problem?\n",
    "   - to what extent is this an actual problem?\n",
    "       \n",
    "    to do this, we    \n",
    "   - define goal(reduce amount of spam messages, or reduce spam complaints)\n",
    "   - goal has to be measurable\n",
    "\n",
    "2. data understanding\n",
    "  - see if the data is available and if it is reliable\n",
    "  - do we have enough data?\n",
    "  - identify data sources (user clicking spam )\n",
    "\n",
    "3. Data prep\n",
    "  - we transform the data so its usable for ML\n",
    "  - clean data, remove noise, build pipelines, convert to tabular form\n",
    "  - we need to extract features and lable them\n",
    "  - for spam, detection, it is converting into the feature matrix\n",
    "\n",
    "4. modeling\n",
    "  - try models and try the best one\n",
    "  - we may go back to data prep to add features or fix data issues\n",
    "\n",
    "5. evaluation\n",
    "  - measure how well this is performing, seeing if it fulfills the business problem\n",
    "  - then we can roll it out to users, stop working on it, go back of it is not fulfilled, etc.\n",
    "\n",
    "6. deployment\n",
    "  - we deploy on the app and roll it out on some users, see if it works well. Then we roll it out\n",
    "  - basically it gets put in production. Focus is not on ml, it is on engineering. \n",
    "\n",
    "then we iterate and keep going and loop back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeac4a9",
   "metadata": {},
   "source": [
    "# Model selection process\n",
    "\n",
    "we do this by taking some data for training, and the rest for testing. The data for testing is called the validation dataset.\n",
    "\n",
    "we compare the results from the validation data with the actual data that is expected. Then we can give it a score.\n",
    "\n",
    "Then we compare those scores to get the best model to use\n",
    "\n",
    "\n",
    "**Multiple comparisons problem (MCP):** just by chance one model can be lucky and obtain\n",
    "good predictions because all of them are probabilistic. \n",
    "\n",
    "The test set can help to avoid the MCP. Obtaining the best model is done with the training and validation datasets, while the test dataset is used for assuring that the proposed best model is the best. \n",
    "\n",
    "1. Split datasets in training, validation, and test. E.g. 60%, 20% and 20% respectively \n",
    "2. Train the models\n",
    "3. Evaluate the models\n",
    "4. Select the best model \n",
    "5. Apply the best model to the test dataset \n",
    "6. Compare the performance metrics of validation and test\n",
    "\n",
    "NB: Note that it is possible to reuse the validation data. After selecting the best model (step 4), the validation and training datasets can be combined to form a single training dataset for the chosen model before testing it on the test set.\n",
    "\n",
    "Later in the video he mentioned to split it into 3: train, validate, and test\n",
    "This way we can train with train data, validate with validate data, and compare it with using the test data to see if the stuff with validate and test are the same.\n",
    "\n",
    "Then we select the best one\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
