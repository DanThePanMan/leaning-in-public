{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91af3bc6",
   "metadata": {},
   "source": [
    "# updated content since there are new libraries to replace the old ones\n",
    "\n",
    "overview:\n",
    " - FastAPI for server\n",
    " - uv for package management\n",
    " - docker for contanerization\n",
    " - fly.io for deployment\n",
    " - sklearn pipelines to make it more managable\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1679e17f",
   "metadata": {},
   "source": [
    "## 1. Save and load the model\n",
    "\n",
    "- assume model is saved already\n",
    "- import pickle so you can save the model in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model.bin', 'wb') as f_out:\n",
    "    pickle.dump((dv, model), f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17991d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load\n",
    "\n",
    "\n",
    "with open('model.bin', 'rb') as f_in:\n",
    "    (dv, model) = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79797ab6",
   "metadata": {},
   "source": [
    "## 2. sklearn pipelines.\n",
    "\n",
    "- It is not conveneient to deal with both dv and modle, so lets ocmbine them into one.\n",
    "- we can do this instead:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d349cb7d",
   "metadata": {},
   "source": [
    "instead of this\n",
    "\n",
    "```\n",
    "\n",
    "dv = DictVectorizer()\n",
    "\n",
    "train_dict = df[categorical + numerical].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "you can do this\n",
    "\n",
    "```\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    DictVectorizer(),\n",
    "    LogisticRegression(solver='liblinear')\n",
    ")\n",
    "\n",
    "pipeline.fit(train_dict, y_train)\n",
    "```\n",
    "\n",
    "and to predict:\n",
    "\n",
    "`pipeline.predict_proba(datapoint)[0, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73689720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now with pickle\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('model.bin', 'wb') as f_out:\n",
    "    pickle.dump(pipeline, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec8168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load\n",
    "\n",
    "\n",
    "with open('model.bin', 'rb') as f_in:\n",
    "    pipeline = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a1e066",
   "metadata": {},
   "source": [
    "## 3. load this whole thing in a python script instead a notebook\n",
    "\n",
    "then we will need to seperate it into a train and a predict file. here are things to take note of:\n",
    "- make the load data, train model, and save model in their own functions, call them, and then log something like \"train complete\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ddf8d7",
   "metadata": {},
   "source": [
    "## 4. Make a FastAPI app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d6784",
   "metadata": {},
   "source": [
    "1. is to install fastapi and uvicorn\n",
    "2. Lets make a simple we app first, and call it ping.py as a proof of concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90364106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the boilerplate\n",
    "\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(title=\"ping\")\n",
    "\n",
    "@app.get(\"/ping\")\n",
    "def ping():\n",
    "    return \"PONG\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=9696)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd877a",
   "metadata": {},
   "source": [
    "to run the app, you can just do `python ping.py`\n",
    "\n",
    "but here is the proper way:\n",
    "\n",
    "`uvicorn ping:app --host 0.0.0.0 --port 9696 --reload`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d44f1c5",
   "metadata": {},
   "source": [
    "you can curl it like this:\n",
    "`curl localhost:9696/ping`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f69971",
   "metadata": {},
   "source": [
    "note that in /docs route in fastapi you can see its docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de9426",
   "metadata": {},
   "source": [
    "## 5. turning the script into a web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b99271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(title=\"customer-churn-prediction\")\n",
    "\n",
    "with open('model.bin', 'rb') as f_in:\n",
    "    pipeline = pickle.load(f_in)\n",
    "\n",
    "\n",
    "def predict_single(customer):\n",
    "    result = pipeline.predict_proba(customer)[0, 1]\n",
    "    return float(result)\n",
    "\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(customer):\n",
    "    prob = predict_single(customer)\n",
    "\n",
    "    return {\n",
    "        \"churn_probability\": prob,\n",
    "        \"churn\": bool(prob >= 0.5)\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=9696)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can add type hinting\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(customer: Dict[str, Any]):\n",
    "    prob = predict_single(customer)\n",
    "\n",
    "    return {\n",
    "        \"churn_probability\": prob,\n",
    "        \"churn\": bool(prob >= 0.5)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf20e59",
   "metadata": {},
   "source": [
    "## 6. Make a script to test this\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b7c2aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'churn_probability': 0.6638108546481684, 'churn': True}\n",
      "customer is likely to churn, send promo\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:9696/predict'\n",
    "\n",
    "customer = {\n",
    "    'gender': 'female',\n",
    "    'seniorcitizen': 0,\n",
    "    'partner': 'yes',\n",
    "    'dependents': 'no',\n",
    "    'phoneservice': 'no',\n",
    "    'multiplelines': 'no_phone_service',\n",
    "    'internetservice': 'dsl',\n",
    "    'onlinesecurity': 'no',\n",
    "    'onlinebackup': 'yes',\n",
    "    'deviceprotection': 'no',\n",
    "    'techsupport': 'no',\n",
    "    'streamingtv': 'no',\n",
    "    'streamingmovies': 'no',\n",
    "    'contract': 'month-to-month',\n",
    "    'paperlessbilling': 'yes',\n",
    "    'paymentmethod': 'electronic_check',\n",
    "    'tenure': 1,\n",
    "    'monthlycharges': 29.85,\n",
    "    'totalcharges': 29.85\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=customer)\n",
    "predictions = response.json()\n",
    "\n",
    "print(predictions)\n",
    "if predictions['churn']:\n",
    "    print('customer is likely to churn, send promo')\n",
    "else:\n",
    "    print('customer is not likely to churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d63d223",
   "metadata": {},
   "source": [
    "## 7. Pydantic and validation\n",
    "\n",
    "fastapi comes with pydantic. but we can add this bit of code in our app to be more explicit with the input and output type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5147ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Customer(BaseModel):\n",
    "    gender: Literal[\"male\", \"female\"]\n",
    "    seniorcitizen: Literal[0, 1]\n",
    "    partner: Literal[\"yes\", \"no\"]\n",
    "    dependents: Literal[\"yes\", \"no\"]\n",
    "    phoneservice: Literal[\"yes\", \"no\"]\n",
    "    multiplelines: Literal[\"no\", \"yes\", \"no_phone_service\"]\n",
    "    internetservice: Literal[\"dsl\", \"fiber_optic\", \"no\"]\n",
    "    onlinesecurity: Literal[\"no\", \"yes\", \"no_internet_service\"]\n",
    "    onlinebackup: Literal[\"no\", \"yes\", \"no_internet_service\"]\n",
    "    deviceprotection: Literal[\"no\", \"yes\", \"no_internet_service\"]\n",
    "    techsupport: Literal[\"no\", \"yes\", \"no_internet_service\"]\n",
    "    streamingtv: Literal[\"no\", \"yes\", \"no_internet_service\"]\n",
    "    streamingmovies: Literal[\"no\", \"yes\", \"no_internet_service\"]\n",
    "    contract: Literal[\"month-to-month\", \"one_year\", \"two_year\"]\n",
    "    paperlessbilling: Literal[\"yes\", \"no\"]\n",
    "    paymentmethod: Literal[\n",
    "        \"electronic_check\",\n",
    "        \"mailed_check\",\n",
    "        \"bank_transfer_(automatic)\",\n",
    "        \"credit_card_(automatic)\",\n",
    "    ]\n",
    "    tenure: int = Field(..., ge=0)\n",
    "    monthlycharges: float = Field(..., ge=0.0)\n",
    "    totalcharges: float = Field(..., ge=0.0)\n",
    "\n",
    "\n",
    "class PredictResponse(BaseModel):\n",
    "    churn_probability: float\n",
    "    churn: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9787402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we can do this so that we get a specific type for the input and output\n",
    "@app.post(\"/predict\")\n",
    "def predict(customer: Customer) -> PredictResponse:\n",
    "    prob = predict_single(customer.model_dump())\n",
    "\n",
    "    return PredictResponse(\n",
    "        churn_probability=prob,\n",
    "        churn=prob >= 0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a163b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make it so that it raises an error when there is an issue\n",
    "\n",
    "from pydantic import ConfigDict\n",
    "\n",
    "\n",
    "class Customer(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "    ... # rest of the fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c372e",
   "metadata": {},
   "source": [
    "## 8. Environment management with uv\n",
    "\n",
    "uv is a tool for dependency and environment management built with rust. This makes it way faster than anything else\n",
    "\n",
    "to install: `pip install uv`\n",
    "to initialize: `uv init`\n",
    "\n",
    "after init, we will realize that we don't need main.py, so we can remove it with rm main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39773839",
   "metadata": {},
   "source": [
    "now we will have a pyproject.toml file. This file can be used to define our dependencies and such. \n",
    "\n",
    "to add dependencies, use` uv add dependency`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86cd3ae",
   "metadata": {},
   "source": [
    "there will be a new file called uv.lock that has all the dependency trees\n",
    "\n",
    "you can use `uv add --dev dependencyName` to add a dev dependency, and `uv run myNormalRunCommand` like \n",
    "`uv run uvicorn predict:app --host 0.0.0.0 --port 9696 --reload` to run something with the uv environment\n",
    "\n",
    "do `uv sync` when theres a new project. this is simlar to npm install. --locked makes sure the installed stuff is exactly what sin uv.lock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e137b09c",
   "metadata": {},
   "source": [
    "## 9. contanerization\n",
    "\n",
    "sometimes there are system dependencies which are outside of venv that we also need to isolate. This is where we use docker for.\n",
    "\n",
    "having a container makes it so that it is self contained and runs everywhere\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fdffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM python:3.12.1\n",
    "\n",
    "\n",
    "# we always want to start with the python version then install the package manager.\n",
    "RUN pip install uv\n",
    "\n",
    "# then we set the working environment that is inside the container\n",
    "WORKDIR /app\n",
    "\n",
    "\n",
    "# then we copy python version, pyproject, and uvlock into our ./(working directory)\n",
    "COPY .python-version pyproject.toml uv.lock ./\n",
    "\n",
    "# install dependencies\n",
    "RUN uv sync --locked\n",
    "\n",
    "\n",
    "# copy our web service and our model into our working directory\n",
    "COPY main.py model.bin ./\n",
    "\n",
    "# let docker know that we will run something on this port\n",
    "EXPOSE 9696\n",
    "\n",
    "\n",
    "# sets the command to run the web service\n",
    "ENTRYPOINT [ \"uv\", \"run\", \"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\",\"--port\", \"9696\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353c821",
   "metadata": {},
   "source": [
    "Then we need to build and run this\n",
    "\n",
    "`docker build -t predict-churn .`  predict-churn is the tag given to this build, . is the current working directory\n",
    "\n",
    "then we run it \n",
    "`docker run -it --rm -p 9696:9696 predict-churn`\n",
    "- it stands for interactive\n",
    "- rm means we remove the container when we are done\n",
    "- -p 9696 makes the port 9696 on the computer the port 9696 in the environment\n",
    "- predict-churn is the name of the tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606777d4",
   "metadata": {},
   "source": [
    "10. Deploy on fly.io\n",
    "\n",
    "im not adding a credit card but just follow the steps "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
