{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "346dcfbe",
   "metadata": {},
   "source": [
    "# Hands On Project\n",
    "\n",
    "car price prediction model based on feature.\n",
    "\n",
    "We will use the dataset from kaggle\n",
    "https://www.kaggle.com/datasets/CooperUnion/cardataset\n",
    "\n",
    "<br>\n",
    "Here is the pan:\n",
    "- Prepare data and do EDA\n",
    "- Use linear regression\n",
    "- Understand the internals of linear regression\n",
    "- Evaluate with RMSE\n",
    "- Feature engineering\n",
    "- Regularization\n",
    "- Use the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c83332d",
   "metadata": {},
   "source": [
    "## 1: Data Prep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77141f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load data form a csv use pd.read_csv()\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/car-price.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33c88d",
   "metadata": {},
   "source": [
    "notice how there are some inconsistencies for the column names, some have upper case some have under scores. Lets make them lower case and mak ethem lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d11454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower().str.replace(' ','_')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19302df1",
   "metadata": {},
   "source": [
    "Now we normalize the values. First we need to find the non string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328821d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all types\n",
    "df.dtypes\n",
    "\n",
    "# we only care about the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d5318",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "# care about the index, since values are all object\\\n",
    "strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9709026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over to clean them up\n",
    "for col in strings:\n",
    "    df[col] = df[col].str.lower().str.replace(' ','_')\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5391832",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col)\n",
    "    print(df[col].unique()) \n",
    "    print(df[col].nunique()) # see how many unique values there actually are per column\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b3173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we visualize it\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2456800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see the price distribution\n",
    "#bins is the width of each bar\n",
    "sns.histplot(df.msrp[df.msrp < 100000], bins = 50)\n",
    "\n",
    "# we can see msot cars are cheap but there are a few ars are up to the millions.\n",
    "# this is the long tail distribution, by butting the requirement of < 100k we remove the long tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a154471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get rid of this long tail, with the log distribution. \n",
    "\n",
    "np.log([1, 10, 1000, 100000]) \n",
    "# you can see it brings high values and bring it lower\n",
    "# we cannot log 0\n",
    "#it is common to add one to each item in the list, insetad of log it is log1p\n",
    "\n",
    "np.log1p([1, 10, 1000, 100000]) \n",
    "\n",
    "#use this on the prices\n",
    "\n",
    "price_logs = np.log1p(df.msrp)\n",
    "sns.histplot(price_logs, bins = 50) #notice how the tail is gone. Now this is normal dist \n",
    "#models love normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f60e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gettinf rid of missing values\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e04b4",
   "metadata": {},
   "source": [
    "# setting up validation framework to validate the model\n",
    "\n",
    "as mentioend before, we need to take our model to use some ata fro training, some for validatoin, and some for testing.\n",
    "\n",
    "lets do 60 20 20 split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2730510",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "\n",
    "n_val = int(n*0.2)\n",
    "n_test = int(n*0.2)\n",
    "n_train = n - n_val - n_test\n",
    "#this way there are no rounding erros and we use all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "91a66a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets decompose the df, we can use oloc\n",
    "\n",
    "\n",
    "df_train = df.iloc[:n_train]\n",
    "df_val = df.iloc[n_train:n_train + n_val]\n",
    "df_test = df.iloc[n_train+ n_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "77165cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7150, 2382, 2382)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to shuffle the records so it distributes evenly\n",
    "# here is how we do it\n",
    "\n",
    "idx = np.arange(n)\n",
    "np.random.seed(2) # so that it is reproducable\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "\n",
    "df_train = df.iloc[idx[:n_train]]\n",
    "df_val = df.iloc[idx[n_train:n_train + n_val]]\n",
    "df_test = df.iloc[idx[n_train+ n_val:]]\n",
    "\n",
    "#now we are getting it through the index instad of directly using the iloc\n",
    "\n",
    "\n",
    "# check to make sure the numbers are right\n",
    "len(df_train), len(df_val), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "109fcd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we reset the index column \n",
    "\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test = df_train.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb624323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the target variables (y)\n",
    "\n",
    "df\n",
    "y_train = np.log1p(df_train.msrp.values)\n",
    "y_val = np.log1p(df_val.msrp.values)\n",
    "y_test = np.log1p(df_test.msrp.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3793a086",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove the values from original df\n",
    "\n",
    "del df_train['msrp']\n",
    "del df_val['msrp']\n",
    "del df_test['msrp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a2db0",
   "metadata": {},
   "source": [
    "# Linear regression!!!\n",
    "\n",
    "this is a model for solving regression tasks, in which the objective is to adjust a line for the data and make predictions on new values. The input is the feature matrix X, and a y vector of predictions is obtained. Trying to be as close as possible to the actual y values. \n",
    "\n",
    "<br>\n",
    "\n",
    "the formula is the sum of the feature times their corrisponding weight so x1*w1+x2*w2 etc.\n",
    "\n",
    "<br>\n",
    "there is a function g that takes in all the features and produces something similar to y\n",
    "\n",
    "So the simple linear regression formula looks like:\n",
    "\n",
    "$g(x_i) = w_0 + x_{i1} \\cdot w_1 + x_{i2} \\cdot w_2 + ... + x_{in} \\cdot w_n$.\n",
    "\n",
    "And that can be further simplified as:\n",
    "\n",
    "$g(x_i) = w_0 + \\displaystyle\\sum_{j=1}^{n} w_j \\cdot x_{ij}$\n",
    "\n",
    "Here is a simple implementation of Linear Regression in python:\n",
    "\n",
    "~~~~python\n",
    "w0 = 7.1\n",
    "def linear_regression(xi):\n",
    "    \n",
    "    n = len(xi)\n",
    "    \n",
    "    pred = w0\n",
    "    w = [0.01, 0.04, 0.002]\n",
    "    for j in range(n):\n",
    "        pred = pred + w[j] * xi[j]\n",
    "    return pred\n",
    "~~~~\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "64862f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are gonna use the training data to run this\n",
    "df_train.iloc[10]\n",
    "\n",
    "# lets take 3 params for now,hp, city mpg, and popularity\n",
    "xi = [453, 11, 86]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f453a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we get the intercept and the weights:\n",
    "w0 = 7.17 # intercept\n",
    "w = [0.01, 0.04, 0.002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7295fbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the g function\n",
    "def linear_regression(xi):\n",
    "    n = len(xi)\n",
    "    \n",
    "    pred = w0\n",
    "    for j in range(n):\n",
    "        pred = pred + w[j] * xi[j]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "23fb788a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.312"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution = linear_regression(xi)\n",
    "# right now it makes no sense bcause the weights are all arbitrary values\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf305cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(222347.2221101062)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since we did log dist, the result is jus the exponent, we need to get the exponent \n",
    "# we did minus one here since we added one to everything in the log to prevent log 0\n",
    "np.expm1(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486e139b",
   "metadata": {},
   "source": [
    "# Liner regression vector form\n",
    "\n",
    "\n",
    "The formula of linear regression can be synthesized with the dot product between features and weights. The feature vector includes the *bias* term with an *x* value of one, such as $w_{0}^{x_{i0}},\\ where\\ x_{i0} = 1\\ for\\ w_0$.\n",
    "\n",
    "When all the records are included, the linear regression can be calculated with the dot product between ***feature matrix*** and ***vector of weights***, obtaining the `y` vector of predictions. \n",
    "\n",
    "\n",
    "**to put it in human words, it is basically the same thing as the one we did before, but instaed of multiplying numbers we are getting the dot product for vectors**\n",
    "\n",
    "<br>\n",
    "notes: https://knowmledge.com/2023/09/20/ml-zoomcamp-2023-machine-learning-for-regression-part-5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "798039e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the dot product function\n",
    "def dot(xi, w):\n",
    "    n = len(xi)\n",
    "    res = 0.0\n",
    "    \n",
    "    \n",
    "    for j in range(n):\n",
    "        res = res + xi[j] * w[j]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6efcdebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regerssoin\n",
    "def linear_regression(xi):\n",
    "    return w0 + dot(xi, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0e078b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to simplify the equatoin, we can imagine there is one more feaure xi0 that is always equal to one, this way we  do  not have to leave w0 all alone\n",
    "w_new = [w0] + w\n",
    "def linear_regression(xi):\n",
    "    xi = [1] + xi\n",
    "    return dot(xi, w_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347f1d5",
   "metadata": {},
   "source": [
    "## here is a small example of how this calculation works\n",
    "\n",
    "We have a matrix $X \\in \\mathbb{R}^{m \\times (n+1)}$ and weight vector $w \\in \\mathbb{R}^{(n+1) \\times 1}$:\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & \\cdots & x_{1n} \\\\\n",
    "1 & x_{21} & \\cdots & x_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{m1} & \\cdots & x_{mn}\n",
    "\\end{bmatrix}, \n",
    "\\quad\n",
    "w =\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Predictions are:\n",
    "\n",
    "$$\n",
    "y_{\\text{pred}} = X w\n",
    "$$\n",
    "\n",
    "Each element is a dot product of a row of $X$ with $w$:\n",
    "\n",
    "$$\n",
    "y_{\\text{pred},i} = x_i^T w = \\sum_{j=0}^{n} x_{ij} w_j\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}1 & 2 \\\\ 1 & 3 \\\\ 1 & 4 \\end{bmatrix}, \\quad\n",
    "w = \\begin{bmatrix}0.5 \\\\ 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Step-by-step calculation:\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "y_{\\text{pred},1} &= 1 \\cdot 0.5 + 2 \\cdot 2 = 0.5 + 4 = 4.5 \\\\\n",
    "y_{\\text{pred},2} &= 1 \\cdot 0.5 + 3 \\cdot 2 = 0.5 + 6 = 6.5 \\\\\n",
    "y_{\\text{pred},3} &= 1 \\cdot 0.5 + 4 \\cdot 2 = 0.5 + 8 = 8.5\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "So the prediction vector is:\n",
    "\n",
    "$$\n",
    "y_{\\text{pred}} = \\begin{bmatrix}4.5 \\\\ 6.5 \\\\ 8.5\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af04438",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The general formula, as mentioned above, is:\n",
    "\n",
    "g(xi) = W0 + summation(1,n) (w[j] * xi[j])  \n",
    "However, it can be written as another form. Specifically if we look at (w[j] * xi[j]):\n",
    "\n",
    "g(xi) = w0 + xi^T * W  \n",
    "whereby xi^T is the transpose of xi.\n",
    "\n",
    "Why is this so?\n",
    "\n",
    "- Both the weights and feature matrices are vectors, with size (n,1) where n is the number of features.\n",
    "- The number of weights equals the number of features, so they both have the same size.\n",
    "- For vector-vector multiplication to occur, the first vector needs to have the same number of columns as the number of rows of the second vector.\n",
    "- We can either transpose the weights or the feature matrix.\n",
    "- In this case, we transpose xi so that we get a matrix with size (1,n).\n",
    "- Since we want to get the inner product, we use xi^T W to get a product of (1,1), which is the prediction (instead of W xi^T).\n",
    "- The vector-vector multiplication occurs, and we get the prediction.\n",
    "- Note: We can transpose either vector, but since we're transposing xi, we must ensure we change the position of the matrices, as the position affects the product in matrix multiplication (or in this case, vector-vector multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e893da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the same data as an example, but with a matrix instad. Here is the base data \n",
    "w0 = 7.17 # intercept\n",
    "xi = [453, 11, 86]\n",
    "w = [0.01, 0.04, 0.002]\n",
    "w_new = [w0] + w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7673299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we modify it to make an example of actual data that we used\n",
    "\n",
    "x1 = [1, 148, 24, 1385] # notice how we have the 1 to make it so that we dont even out the w\n",
    "x2 = [1, 132, 25, 2031]\n",
    "x10 = [1, 453, 11, 86]\n",
    "\n",
    "X = [x1, x2, x10]\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e400b1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.38 , 13.552, 12.312])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dot(w_new) # this is the results of the linear regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
